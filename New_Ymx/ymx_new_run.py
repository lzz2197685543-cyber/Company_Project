from dingding_doc import DingTalkSheetUploader,DingTalkTokenManager
from logger_config import SimpleLogger
from ymx_new_data_multithread import NewYmxNewData
from datetime import datetime
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, current_thread
from login import MaiJiaLogin
import asyncio
from filter_new_data import DataProcessor


async def main():
    client = MaiJiaLogin(
        phone="BAK2023",
        password="lxz2026",
        headless=False
    )
    await client.login_and_save_cookie_dict("./data/sellersprite_cookie_dict.json")



def crawl_country(country_name, file_lock):
    """çº¿ç¨‹ä»»åŠ¡å‡½æ•°ï¼šçˆ¬å–å•ä¸ªå›½å®¶"""
    thread_name = current_thread().name
    print(f"[{thread_name}] å¼€å§‹çˆ¬å–{country_name}ç«™ç‚¹")

    try:
        ymx = NewYmxNewData(file_lock=file_lock)
        ymx.set_country(country_name)
        total_items = ymx.get_all_page(start_page=1, max_page=1000)

        print(f"[{thread_name}] âˆš {country_name}: çˆ¬å–æˆåŠŸï¼Œè·å–{total_items}æ¡æ•°æ®")
        return {
            "thread": thread_name,
            "country": country_name,
            "status": "success",
            "total_items": total_items
        }
    except Exception as e:
        print(f"[{thread_name}] Ã— {country_name}: çˆ¬å–å¤±è´¥ï¼Œé”™è¯¯: {e}")
        return {
            "thread": thread_name,
            "country": country_name,
            "status": "error",
            "error": str(e)
        }

def ymx_main_thread_pool(max_workers=3):
    """ä½¿ç”¨çº¿ç¨‹æ± çš„ä¸»ç¨‹åºå…¥å£"""
    print("=" * 60)
    print("äºšé©¬é€Šå•†å“çˆ¬è™«å¯åŠ¨ï¼ˆçº¿ç¨‹æ± ç‰ˆï¼‰")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"çº¿ç¨‹æ± å¤§å°: {max_workers}")
    print(f"ç›®æ ‡å›½å®¶: ç¾å›½ã€è‹±å›½ã€å¾·å›½ã€æ³•å›½ã€è¥¿ç­ç‰™")
    print("=" * 60)
    print("åˆ†é…ä»»åŠ¡ä¸­...")

    countries = ["ç¾å›½", "è‹±å›½", "å¾·å›½", "æ³•å›½", "è¥¿ç­ç‰™"]

    # åˆ›å»ºæ–‡ä»¶é”ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨åœ°å†™å…¥æ–‡ä»¶
    file_lock = Lock()

    # ç»Ÿè®¡ä¿¡æ¯
    success_count = 0
    error_count = 0
    total_items_all = 0
    thread_results = {}

    print("\nå¯åŠ¨çº¿ç¨‹æ± ï¼Œå¼€å§‹å¹¶å‘çˆ¬å–...")
    print("-" * 60)

    # åˆ›å»ºçº¿ç¨‹æ± 
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="YmxThread") as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_country = {
            executor.submit(crawl_country, country, file_lock): country
            for country in countries
        }

        # æ˜¾ç¤ºçº¿ç¨‹åˆ†é…ä¿¡æ¯
        print(f"ä»»åŠ¡åˆ†é…å®Œæˆ:")
        for future, country in future_to_country.items():
            print(f"  - {country} -> å·²æäº¤åˆ°çº¿ç¨‹æ± ")

        print("\nç­‰å¾…ä»»åŠ¡æ‰§è¡Œ...")
        print("-" * 60)

        # ç­‰å¾…ä»»åŠ¡å®Œæˆå¹¶å¤„ç†ç»“æœ
        completed_count = 0
        for future in as_completed(future_to_country):
            completed_count += 1
            country = future_to_country[future]

            try:
                result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                thread_name = result.get("thread", "æœªçŸ¥çº¿ç¨‹")

                if result["status"] == "success":
                    success_count += 1
                    total_items = result.get("total_items", 0)
                    total_items_all += total_items
                    thread_results[thread_name] = {
                        "country": country,
                        "status": "æˆåŠŸ",
                        "items": total_items
                    }
                    print(
                        f"[è¿›åº¦ {completed_count}/{len(countries)}] {thread_name}: âˆš {country} å®Œæˆï¼Œè·å–{total_items}æ¡æ•°æ®")
                else:
                    error_count += 1
                    thread_results[thread_name] = {
                        "country": country,
                        "status": "å¤±è´¥",
                        "error": result.get("error", "æœªçŸ¥é”™è¯¯")
                    }
                    print(f"[è¿›åº¦ {completed_count}/{len(countries)}] {thread_name}: Ã— {country} å¤±è´¥")

            except Exception as e:
                error_count += 1
                print(f"[è¿›åº¦ {completed_count}/{len(countries)}] å¤„ç†{country}ç»“æœæ—¶å‡ºé”™: {e}")

    print("\n" + "=" * 60)
    print("æ‰€æœ‰å›½å®¶çˆ¬å–å®Œæˆï¼")
    print("=" * 60)

    # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    print("\nè¯¦ç»†ç»Ÿè®¡:")
    print("-" * 40)
    for thread_name, result in thread_results.items():
        if result["status"] == "æˆåŠŸ":
            print(f"{thread_name}: {result['country']} - {result['status']} ({result['items']}æ¡æ•°æ®)")
        else:
            print(f"{thread_name}: {result['country']} - {result['status']} ({result.get('error', 'æœªçŸ¥é”™è¯¯')})")

    print("-" * 40)
    print(f"æ€»ç»“:")
    print(f"  æˆåŠŸ: {success_count}ä¸ªå›½å®¶")
    print(f"  å¤±è´¥: {error_count}ä¸ªå›½å®¶")
    print(f"  æ€»æ•°æ®é‡: {total_items_all}æ¡")
    print(f"  ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # ä¿å­˜ç»Ÿè®¡ç»“æœåˆ°æ–‡ä»¶
    save_statistics(thread_results, total_items_all, success_count, error_count)

def save_statistics(thread_results, total_items, success_count, error_count):
    """ä¿å­˜çˆ¬å–ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶"""
    current_date = datetime.now().strftime("%Y%m%d")
    stats_file = f"./logs/ymx_stats_{current_date}.txt"

    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("äºšé©¬é€Šå•†å“çˆ¬è™«ç»Ÿè®¡æŠ¥å‘Š\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 60 + "\n\n")

        f.write("çº¿ç¨‹æ‰§è¡Œè¯¦æƒ…:\n")
        f.write("-" * 40 + "\n")
        for thread_name, result in thread_results.items():
            if result["status"] == "æˆåŠŸ":
                f.write(f"{thread_name}: {result['country']} - {result['status']} ({result['items']}æ¡æ•°æ®)\n")
            else:
                f.write(
                    f"{thread_name}: {result['country']} - {result['status']} ({result.get('error', 'æœªçŸ¥é”™è¯¯')})\n")

        f.write("\n" + "-" * 40 + "\n")
        f.write(f"æ€»ç»“ç»Ÿè®¡:\n")
        f.write(f"  æˆåŠŸå›½å®¶æ•°: {success_count}\n")
        f.write(f"  å¤±è´¥å›½å®¶æ•°: {error_count}\n")
        f.write(f"  æ€»æ•°æ®é‡: {total_items}æ¡\n")
        f.write("=" * 60 + "\n")

    print(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")


def upload_multiple_records(config, records):
    """
    æ‰¹é‡ä¸Šä¼ å¤šæ¡è®°å½• - ä¿®å¤NaNé—®é¢˜ç‰ˆ
    """
    token_manager = DingTalkTokenManager()
    uploader = DingTalkSheetUploader(
        base_id=config["base_id"],
        sheet_id=config["sheet_id"],
        operator_id=config["operator_id"],
        token_manager=token_manager
    )

    logger.info(f"å‡†å¤‡ä¸Šä¼  {len(records)} æ¡è®°å½•...")

    # å…³é”®ä¿®å¤ï¼šå¤„ç†NaNå€¼
    import math

    def fix_nan(obj):
        if isinstance(obj, float):
            if math.isnan(obj) or math.isinf(obj):
                return None
            # å¤§æ—¶é—´æˆ³è½¬ä¸ºå­—ç¬¦ä¸²
            elif obj > 1e12:
                from datetime import datetime
                try:
                    return datetime.fromtimestamp(obj / 1000).strftime("%Y-%m-%d")
                except:
                    return str(obj)
        return obj

    # é¢„å¤„ç†æ‰€æœ‰è®°å½•
    processed_records = []
    for record in records:
        new_record = {}
        for key, value in record.items():
            if isinstance(value, dict):
                new_record[key] = {k: fix_nan(v) for k, v in value.items()}
            else:
                new_record[key] = fix_nan(value)
        processed_records.append(new_record)

    logger.info(f"å·²å®Œæˆæ•°æ®é¢„å¤„ç†ï¼Œä¿®å¤äº†NaNå’ŒInfinityå€¼")

    # ä¸Šä¼ 
    results = uploader.upload_batch_records(processed_records, batch_size=50, delay=0.2, max_retries=2)

    # åˆ†æç»“æœ
    successful = [r for r in results if r.get("success")]

    logger.info(f"\nä¸Šä¼ ç»Ÿè®¡:")
    logger.info(f"æ€»æ‰¹æ¬¡: {len(results)}")
    logger.info(f"æˆåŠŸæ‰¹æ¬¡: {len(successful)}")
    logger.info(f"å¤±è´¥æ‰¹æ¬¡: {len(results) - len(successful)}")

    # å¦‚æœè¿˜æœ‰å¤±è´¥ï¼Œä¿å­˜è¿™äº›è®°å½•
    if len(successful) < len(results):
        failed_records = []
        for i, result in enumerate(results):
            if not result.get("success"):
                start_idx = i * 50
                end_idx = min(start_idx + 50, len(processed_records))
                failed_records.extend(processed_records[start_idx:end_idx])

        if failed_records:
            import json
            import os
            from datetime import datetime

            os.makedirs("failed_records", exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join("failed_records", f"final_failed_{timestamp}.json")

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(failed_records, f, ensure_ascii=False, indent=2)

            logger.info(f"ä»æœ‰ {len(failed_records)} æ¡è®°å½•å¤±è´¥ï¼Œå·²ä¿å­˜åˆ°: {filepath}")

    return results


def save_failed_records(failed_records):
    """
    ç®€æ´åœ°ä¿å­˜å¤±è´¥è®°å½•åˆ°æ–‡ä»¶
    """
    import json
    import os
    from datetime import datetime

    # åˆ›å»ºç›®å½•
    os.makedirs("./data", exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"failed_records_{timestamp}.json"
    filepath = os.path.join("failed_records", filename)

    try:
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(failed_records, f, ensure_ascii=False, indent=2, default=str)

        logger.info(f"å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    except Exception as e:
        logger.error(f"ä¿å­˜å¤±è´¥è®°å½•å¤±è´¥: {e}")
        return None




if __name__ == '__main__':

    logger = SimpleLogger(name='run')
    logger.info('ç¨‹åºå¼€å§‹å¯åŠ¨')

    config = {
        "base_id": "KGZLxjv9VG03dPLZt4B3yZgjJ6EDybno",
        "sheet_id": "ç”µå•†å¹³å°é€‰å“1",
        "operator_id": "ZiSpuzyA49UNQz7CvPBUvhwiEiE"
    }

    # è®°å½•æ€»æ—¶é—´å¼€å§‹
    total_start_time = time.time()

    logger.info('---------------------------------å¼€å§‹ç™»å½•è·å–cookies-----------------------------------')
    asyncio.run(main())


    logger.info('---------------------------------å¼€å§‹çˆ¬å–temu_newæ•°æ®-----------------------------------')
    ymx_main_thread_pool(max_workers=5)  # å¯ä»¥è°ƒæ•´çº¿ç¨‹æ•°

    logger.info('---------------------------------å¼€å§‹å»é‡æ•°æ®-----------------------------------')
    processor = DataProcessor()

    # ç­›é€‰æ–°æ•°æ®
    new_data = processor.filter_new_data()

    logger.info('---------------------------------å¼€å§‹æ„å»ºä¸Šä¼ çš„æ•°æ®-----------------------------------')
    records = processor.build_records(new_data)

    processor.import_csv_to_product_monitor(new_data)

    logger.info('---------------------------------å¼€å§‹ä¸Šä¼ æ•°æ®-----------------------------------')
    upload_multiple_records(config, records)

    logger.info(f'æ•°æ®ä¸Šä¼ æˆåŠŸ')

    time.sleep(3)


    # è®¡ç®—æ€»æ—¶é—´
    total_time = time.time() - total_start_time

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logger.info(f"{'=' * 60}")
    logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
    logger.info(f"â±ï¸  å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_start_time))}")
    logger.info(f"â±ï¸  ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))}")
